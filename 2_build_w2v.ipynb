{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. word2vec モデル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from janome.tokenizer import Tokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "f_dir = '/root/userspace/dl4us_final_report/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_docs = pickle.load(open(f_dir + 'pickles/shf_docs.pickle', 'rb'))\n",
    "shf_labels = pickle.load(open(f_dir + 'pickles/shf_labels.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs done\n",
      "2000 docs done\n",
      "4000 docs done\n",
      "6000 docs done\n",
      "8000 docs done\n",
      "10000 docs done\n",
      "12000 docs done\n",
      "14000 docs done\n",
      "16000 docs done\n",
      "18000 docs done\n",
      "20000 docs done\n",
      "22000 docs done\n",
      "24000 docs done\n",
      "26000 docs done\n",
      "28000 docs done\n",
      "28222 : total docs\n",
      "975010 : total sentences\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []  # 保存データは文書単位（文書x行x単語）\n",
    "sentences = []  # word2vecには行単位のデータを渡す（全文書行x単語）\n",
    "for i, ((cat, lang), doc) in enumerate(zip(shf_labels, shf_docs)):\n",
    "    # 英語は nltk.word_tokenize, 日本語は janome.Tokanizer でtoken化\n",
    "    if lang == 'en':\n",
    "        tkzd_doc = [word_tokenize(s) for s in doc.split('\\n')]\n",
    "    elif lang == 'ja':\n",
    "        t = Tokenizer()\n",
    "        tkzd_doc = [t.tokenize(s, wakati=True) for s in doc.split('\\n')]\n",
    "    else:\n",
    "        print('lang:', lang, file=sys.stderr)\n",
    "        sys.exit()\n",
    "\n",
    "    tokenized_docs.append(tkzd_doc)\n",
    "    sentences.extend(tkzd_doc)\n",
    "\n",
    "    # 経過表示\n",
    "    if i % 2000 == 0:\n",
    "        print(i, 'docs done')\n",
    "\n",
    "print(len(tokenized_docs), ': total docs')\n",
    "print(len(sentences), ': total sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 tokenize データ保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenized_docs, open(f_dir + 'pickles/shf_tkn_docs.pickle', 'wb'))\n",
    "pickle.dump(sentences, open(f_dir + 'pickles/sentences_for_w2v.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 word2vec 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 400\n",
    "model = Word2Vec(sentences, size=size, sg=1, min_count=1)\n",
    "model.save(f_dir + 'models/w2v_' + str(size) + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "街道 :\n",
      "('京街道', 0.7725939154624939)\n",
      "('中山道', 0.7632403373718262)\n",
      "('東高野', 0.7380308508872986)\n",
      "('宿場', 0.7368693351745605)\n",
      "('難所', 0.731268584728241)\n",
      "('畿七道', 0.7284597158432007)\n",
      "('旧道', 0.7209132313728333)\n",
      "('堤上', 0.7207267880439758)\n",
      "('沿い', 0.7183176279067993)\n",
      "('左岸', 0.7145272493362427)\n",
      "\n",
      "近い :\n",
      "('とどまる', 0.7481799125671387)\n",
      "('近かっ', 0.7400604486465454)\n",
      "('つなげる', 0.7357017993927002)\n",
      "('横ばい', 0.7324842214584351)\n",
      "('劣る', 0.7293784618377686)\n",
      "('あわせる', 0.7240008115768433)\n",
      "('青みがかり', 0.722811222076416)\n",
      "('遠い', 0.7224317193031311)\n",
      "('接する', 0.7222763299942017)\n",
      "('ないしは', 0.7221167683601379)\n",
      "\n",
      "castle :\n",
      "('osaka-jo', 0.6921572685241699)\n",
      "('castles', 0.674949586391449)\n",
      "('castellan', 0.6721668243408203)\n",
      "('nagoya-jo', 0.6708797216415405)\n",
      "('edo-jo', 0.6590505838394165)\n",
      "('fushimi-jo', 0.6575532555580139)\n",
      "('himeji-jo', 0.6512061953544617)\n",
      "('sawayama-jo', 0.6481110453605652)\n",
      "('kumamoto-jo', 0.6436932682991028)\n",
      "('kannonji-jo', 0.6383404731750488)\n",
      "\n",
      "long :\n",
      "('short', 0.6913972496986389)\n",
      "('longest', 0.6104753613471985)\n",
      "('shorter', 0.6046383380889893)\n",
      "('ago', 0.6027876138687134)\n",
      "('longer', 0.6018555164337158)\n",
      "('kalpa', 0.5998243093490601)\n",
      "('san-shaku', 0.5992670655250549)\n",
      "('90.9', 0.5960545539855957)\n",
      "('90cm', 0.595517098903656)\n",
      "('20cm', 0.5948607921600342)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word2vec 確認\n",
    "words = ('街道', '近い', 'castle', 'long')\n",
    "for word in words:\n",
    "    print(word, ':')\n",
    "    out = model.most_similar(positive=[word])\n",
    "    for x in out:\n",
    "        print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
